{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsdb\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "force = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s: np.ndarray) -> list[float]:\n",
    "    return ((s - s.mean()) / np.std(s)).tolist()\n",
    "\n",
    "\n",
    "def save(data: np.ndarray,\n",
    "         to: str):\n",
    "    assert isinstance(data, list)\n",
    "    assert isinstance(data[0], list)\n",
    "    assert isinstance(data[0][0], float)\n",
    "    with open(to, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def split(data: list[np.ndarray],\n",
    "          train: float = .8,\n",
    "          val: float = .1) -> tuple[list[np.ndarray],\n",
    "                                    list[np.ndarray],\n",
    "                                    list[np.ndarray]]:\n",
    "    \"\"\"Generate a train/validation/test split.\"\"\"\n",
    "    if len(data) > 10:\n",
    "        p_train = int(len(data) * train)\n",
    "        p_val = int(len(data) * val)\n",
    "    else:\n",
    "        p_val = 1\n",
    "        p_train = len(data) - 2\n",
    "    return data[:p_train], data[p_train:p_train+p_val], data[p_train+p_val:]\n",
    "\n",
    "\n",
    "def split_2way(data: list[np.ndarray],\n",
    "          train: float = .9) -> tuple[list[np.ndarray],\n",
    "                                      list[np.ndarray]]:\n",
    "    \"\"\"Generate a train/test split.\"\"\"\n",
    "    p = int(len(data) * train)\n",
    "    return data[:p], data[p:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tsdb(s: str):\n",
    "    tsdb.utils.logging.logger.setLevel(logging.ERROR)\n",
    "    raw = tsdb.load_dataset(s)\n",
    "    if s == \"electricity_load_diagrams\":\n",
    "        raw = (raw[\"X\"].select_dtypes(include=[np.number])\n",
    "                       .values.T)\n",
    "        raw = [normalize(s) for s in raw]\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in [\"electricity_load_diagrams\"]:\n",
    "    dataset = download_tsdb(dataset_name)\n",
    "    dataset_normalized = map(normalize, dataset)\n",
    "    dataset_train,dataset_validate, dataset_test = split(dataset)\n",
    "    save(dataset_train,\n",
    "         f\"../data/processed/{dataset_name}_TRAIN.pickle\")\n",
    "    save(dataset_validate,\n",
    "         f\"../data/processed/{dataset_name}_VAL.pickle\")\n",
    "    save(dataset_test,\n",
    "         f\"../data/processed/{dataset_name}_TEST.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.datasets import load_regression\n",
    "\n",
    "def download_tser(name: str) -> None:\n",
    "    try:\n",
    "        load_regression(name,\n",
    "                        extract_path=\"../data/raw/tser\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "for tser_dataset in [\"HouseholdPowerConsumption1\",\n",
    "                     \"HouseholdPowerConsumption2\"]:\n",
    "    download_tser(tser_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HouseholdPowerConsumption2_TEST\n",
      "688\n",
      "../data/raw/tser/HouseholdPowerConsumption2/HouseholdPowerConsumption2_TEST.ts\n",
      "HouseholdPowerConsumption2_TRAIN\n",
      "HouseholdPowerConsumption1_TEST\n",
      "688\n",
      "../data/raw/tser/HouseholdPowerConsumption1/HouseholdPowerConsumption1_TEST.ts\n",
      "HouseholdPowerConsumption1_TRAIN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def loads_ts(file: str):\n",
    "    lines = [line for line in open(file)\n",
    "             if not (line.startswith(\"#\") or line.startswith(\"@\"))]\n",
    "    series = list()\n",
    "    for line in lines:\n",
    "        channels = line.split('):(')\n",
    "        for channel in channels:\n",
    "            data = re.findall(r\",(\\d+\\.\\d+)\\),\", channel)\n",
    "            data = [float(p) for p in data]\n",
    "            if all(v > 0 for v in data):\n",
    "                series.append(normalize(np.array(data)))\n",
    "    return series\n",
    "\n",
    "for dataset_name in Path(\"../data/raw/tser\").glob(\"**/*.ts\"):\n",
    "    print(dataset_name.stem)\n",
    "    dataset = loads_ts(dataset_name)\n",
    "    if dataset_name.stem==\"HouseholdPowerConsumption1_TEST\" or dataset_name.stem==\"HouseholdPowerConsumption2_TEST\":\n",
    "        p = int(len(dataset)/2)\n",
    "        print(p)\n",
    "        print(dataset_name)\n",
    "        save(dataset[:p],\n",
    "         f\"../data/processed_HH/{dataset_name.stem[:-4]}VAL.pickle\")\n",
    "        save(dataset[p:],\n",
    "         f\"../data/processed_HH/{dataset_name.stem}.pickle\")\n",
    "    else:\n",
    "        save(dataset,\n",
    "         f\"../data/processed_HH/{dataset_name.stem}.pickle\")\n",
    "\n",
    "# for dataset_name in Path(\"../data/raw/tser\").glob(\"**/*.ts\"):\n",
    "#     dataset = loads_ts(dataset_name)\n",
    "#     save(dataset,\n",
    "#          f\"../data/processed/{dataset_name.stem}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.datasets import load_forecasting\n",
    "\n",
    "\n",
    "def download_forecasting(name: str):\n",
    "    data = load_forecasting(name,\n",
    "                            extract_path=\"../data/raw/forecasting\",\n",
    "                            return_metadata=False)\n",
    "    data = data[\"series_value\"].values\n",
    "    data = [s.to_numpy() for s in data]\n",
    "    data = [s for s in data if s.sum() > 0 and len(s) > 1024]\n",
    "    data = [normalize(s) for s in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "for dataset_name in [# \"solar_10_minutes_dataset\",\n",
    "                     #\"london_smart_meters_dataset_without_missing_values\",\n",
    "                     # \"australian_electricity_demand_dataset\",\n",
    "                     \"wind_farms_minutely_dataset_without_missing_values\",\n",
    "                     # \"electricity_hourly_dataset\"\n",
    "                     ]:\n",
    "    dataset = download_forecasting(dataset_name)\n",
    "    dataset_train,dataset_val, dataset_test = split(dataset)\n",
    "    save(dataset_train,\n",
    "         f\"../data/processed/{dataset_name}_TRAIN.pickle\")\n",
    "    save(dataset_val,\n",
    "         f\"../data/processed/{dataset_name}_VAL.pickle\")\n",
    "    save(dataset_test,\n",
    "         f\"../data/processed/{dataset_name}_TEST.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['electricity_load_diagrams',\n",
       " 'australian_electricity_demand_dataset',\n",
       " 'HouseholdPowerConsumption2',\n",
       " 'electricity_hourly_dataset',\n",
       " 'wind_farms_minutely_dataset_without_missing_values',\n",
       " 'london_smart_meters_dataset_without_missing_values',\n",
       " 'solar_10_minutes_dataset',\n",
       " 'HouseholdPowerConsumption1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems = [\n",
    "   (p.stem\n",
    "     .removesuffix(\"_TRAIN\")\n",
    "     .removesuffix(\"_TEST\")) for p in Path(\"../data/processed/\").glob(\"*_TEST.pickle\")\n",
    "]\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: config.json\n",
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/workspaces/AICoE_Ramping_Artefacts/artifactory-master/data/processed' 'https://m3mlopssadev.blob.core.windows.net/azureml-blobstore-206414f2-5a5c-4209-8dbe-6d0e233cd920/LocalUpload/8900158fba37b47eeea8e2334ce3e540/processed' \n",
      "\n",
      "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
      "\u001b[32mUploading processed (3676.35 MBs): 100%|██████████| 3676350770/3676350770 [00:12<00:00, 288555586.77it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data({'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_folder', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'train_dataset_tser_tsdb_normalized', 'description': 'collection of datasets from aeon, tser, tsdb normalized with 0 mean and std 1', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/8de3e85d-b97f-48c1-a25b-5bddf9dc484c/resourceGroups/m3-mlops-dev/providers/Microsoft.MachineLearningServices/workspaces/m3-mlops-mlw-dev/data/train_dataset_tser_tsdb_normalized/versions/1', 'Resource__source_path': None, 'base_path': '/workspaces/AICoE_Ramping_Artefacts/artifactory-master/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f740cbb2c10>, 'serialize': <msrest.serialization.Serializer object at 0x7f740cbb01d0>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/8de3e85d-b97f-48c1-a25b-5bddf9dc484c/resourcegroups/m3-mlops-dev/workspaces/m3-mlops-mlw-dev/datastores/workspaceblobstore/paths/LocalUpload/8900158fba37b47eeea8e2334ce3e540/processed/', 'datastore': None})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "#autheticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient.from_config(\n",
    "    credential=credential,\n",
    "    path=\"config.json\",\n",
    ")\n",
    "\n",
    "# Set the version number of the data asset (for example: '1')\n",
    "VERSION = \"1\"\n",
    "\n",
    "# Set the path, supported paths include:\n",
    "# local: './<path>/<folder>' (this will be automatically uploaded to cloud storage)\n",
    "# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<folder>'\n",
    "# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<folder>'\n",
    "# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<folder>'\n",
    "path = \"../data/processed/\"\n",
    "\n",
    "# Define the Data asset object\n",
    "my_data = Data(\n",
    "    path=path,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"collection of datasets from aeon, tser, tsdb normalized with 0 mean and std 1\",\n",
    "    name=\"train_dataset_tser_tsdb_normalized\",\n",
    "    version=VERSION,\n",
    ")\n",
    "\n",
    "# Create the data asset in the workspace\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
