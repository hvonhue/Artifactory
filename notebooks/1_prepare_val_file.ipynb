{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2   \n",
    "\n",
    "import sys\n",
    "sys.path.append('../artitect/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from itertools import repeat\n",
    "from artifact import Saw, Saw_centered, Saw_centered_Francois\n",
    "from data import ArtifactDataset, CachedArtifactDataset, RealisticArtifactDataset, CenteredArtifactDataset\n",
    "from datetime import datetime\n",
    "from sliding_window_detector import SlidingWindowTransformerDetector\n",
    "from utilities import parameters_k\n",
    "import pytz\n",
    "\n",
    "# stop warnings\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing hyperparameters as a dictionary, because we can directly log this config dict to W&B.\n",
    "CONFIG = dict(\n",
    "    # width of window\n",
    "    width = 64,\n",
    "    convolution_features=[256, 128, 64, 32],\n",
    "    convolution_width=[5, 9, 17, 33],\n",
    "    convolution_dropout=0.0,\n",
    "    transformer_heads=2,\n",
    "    transformer_feedforward=128,\n",
    "    transformer_layers=2,\n",
    "    transformer_dropout=0,\n",
    "    loss=\"label\",\n",
    "    loss_boost_fp=0,\n",
    "    \n",
    "    artifact=Saw(),\n",
    "    # Optimizer Parameter\n",
    "\n",
    "    # LearningRate Scheduler\n",
    "    \n",
    "    # parameters for study\n",
    "    batch_size = 32, # 'values': [32, 64, 128]\n",
    "    \n",
    "    wandb_group_name = \"test_setup\",\n",
    "    wandb_project_name = \"artifactory\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file = Path(f\"../data/val_allnoCinECGT{CONFIG['width']}.pkl\")\n",
    "val_datasets = [\n",
    "    \"australian_electricity_demand_dataset\",\n",
    "    \"electricity_hourly_dataset\",\n",
    "    \"electricity_load_diagrams\",\n",
    "    \"HouseholdPowerConsumption1\",\n",
    "    \"london_smart_meters_dataset_without_missing_values\",\n",
    "    \"solar_10_minutes_dataset\",\n",
    "    \"wind_farms_minutely_dataset_without_missing_values\",\n",
    "    'ACSF1',\n",
    "    # 'CinCECGTorso',\n",
    "    'HouseTwenty',\n",
    "    'Mallat',\n",
    "    'MixedShapesRegularTrain',\n",
    "    'Phoneme',\n",
    "    'PigArtPressure',\n",
    "    'PigCVP',\n",
    "    'Rock',\n",
    "    'SemgHandGenderCh2',\n",
    "    'mitbih',\n",
    "    'ptbdb',\n",
    "    'etth',\n",
    "    'ettm'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlidingWindowTransformerDetector_528.96K_04-07-2024_13:30:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'act_fct' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fct'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = SlidingWindowTransformerDetector(window=CONFIG[\"width\"],                    \n",
    "                                  convolution_features=CONFIG[\"convolution_features\"],\n",
    "                                  convolution_width=CONFIG[\"convolution_width\"],\n",
    "                                  convolution_dropout=CONFIG[\"convolution_dropout\"],\n",
    "                                  transformer_heads=CONFIG[\"transformer_heads\"],\n",
    "                                  transformer_feedforward=CONFIG[\"transformer_feedforward\"],\n",
    "                                  transformer_layers=CONFIG[\"transformer_layers\"],\n",
    "                                  transformer_dropout=CONFIG[\"transformer_dropout\"],\n",
    "                                  loss=CONFIG[\"loss\"],\n",
    "                                  loss_boost_fp=CONFIG[\"loss_boost_fp\"])\n",
    "# model = ConvolutionDetector(convolution_features=[128, 64, 32],\n",
    "#                             convolution_width=[5, 9, 33],\n",
    "#                             convolution_dilation=[1, 1, 1],\n",
    "#                             convolution_dropout=0.0,\n",
    "#                             convolution_activation=\"sigmoid\")\n",
    "model_name = f\"{model.__class__.__name__}_{parameters_k(model)}_{datetime.now(pytz.timezone('Europe/Amsterdam')).strftime('%d-%m-%Y_%H:%M:%S')}\"\n",
    "CONFIG['wandb_run_name'] = model_name\n",
    "\n",
    "\n",
    "train_datasets = [\n",
    "    # 'CinCECGTorso', # do not train on this dataset for validation purposes\n",
    "    \"ETTm\",  # 1\n",
    "    \"ETTh\",  # 2\n",
    "    \"electricity_load_diagrams\",  # 3\n",
    "    \"australian_electricity_demand_dataset\",  # 4\n",
    "    \"Phoneme\",  # 5\n",
    "    \"electricity_hourly_dataset\",  # 6\n",
    "    \"HouseholdPowerConsumption1\",  # 7\n",
    "    \"london_smart_meters_dataset_without_missing_values\",  # 8\n",
    "    \"SemgHandGenderCh2\",  # 9\n",
    "    \"PigCVP\",  # 10\n",
    "    \"HouseTwenty\",  # 11\n",
    "    \"wind_farms_minutely_dataset_without_missing_values\",  # 12\n",
    "    \"ptbdb\",  # 13\n",
    "    \"mitbih\",  # 14\n",
    "    \"PigArtPressure\",  # 15\n",
    "    \"solar_10_minutes_dataset\",  # 16\n",
    "    \"Mallat\",  # 17\n",
    "    \"MixedShapesRegularTrain\",  # 18\n",
    "    \"Rock\",  # 19\n",
    "    \"ACSF1\",  # 20\n",
    "]\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_series(names: list[str], split: str, path: str):\n",
    "    series: list[np.ndarray] = list()\n",
    "    counts: list[float] = list()\n",
    "    for name in names:\n",
    "        try:\n",
    "            with open(f\"{path}/{name}_{split}.pickle\", \"rb\") as f:\n",
    "                raw = [a for a in pickle.load(f) if len(a) > CONFIG['width']]\n",
    "                series.extend(np.array(a).astype(np.float32) for a in raw)\n",
    "                counts.extend(repeat(1 / len(raw), len(raw)))\n",
    "        except:\n",
    "            print(f\"Dataset {name} not in input folder!\")\n",
    "    counts = np.array(counts)\n",
    "    return series, np.divide(counts, np.sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ETTm not in input folder!\n",
      "Dataset ETTh not in input folder!\n",
      "Dataset electricity_load_diagrams not in input folder!\n",
      "Dataset australian_electricity_demand_dataset not in input folder!\n",
      "Dataset Phoneme not in input folder!\n",
      "Dataset electricity_hourly_dataset not in input folder!\n",
      "Dataset HouseholdPowerConsumption1 not in input folder!\n",
      "Dataset london_smart_meters_dataset_without_missing_values not in input folder!\n",
      "Dataset HouseTwenty not in input folder!\n",
      "Dataset wind_farms_minutely_dataset_without_missing_values not in input folder!\n",
      "Dataset ptbdb not in input folder!\n",
      "Dataset mitbih not in input folder!\n",
      "Dataset PigArtPressure not in input folder!\n",
      "Dataset solar_10_minutes_dataset not in input folder!\n",
      "Dataset Mallat not in input folder!\n",
      "Dataset MixedShapesRegularTrain not in input folder!\n",
      "Dataset Rock not in input folder!\n",
      "Dataset ACSF1 not in input folder!\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_data, train_weights = load_series(train_datasets, \"TRAIN\", path=\"../data/processed\")\n",
    "train_dataset = CenteredArtifactDataset(train_data,\n",
    "                                width=CONFIG[\"width\"],\n",
    "                                padding=64,\n",
    "                                artifact=CONFIG[\"artifact\"],\n",
    "                                weight=train_weights) \n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset australian_electricity_demand_dataset not in input folder!\n",
      "Dataset electricity_hourly_dataset not in input folder!\n",
      "Dataset electricity_load_diagrams not in input folder!\n",
      "Dataset HouseholdPowerConsumption1 not in input folder!\n",
      "Dataset london_smart_meters_dataset_without_missing_values not in input folder!\n",
      "Dataset solar_10_minutes_dataset not in input folder!\n",
      "Dataset wind_farms_minutely_dataset_without_missing_values not in input folder!\n",
      "Dataset ACSF1 not in input folder!\n",
      "Dataset HouseTwenty not in input folder!\n",
      "Dataset Mallat not in input folder!\n",
      "Dataset MixedShapesRegularTrain not in input folder!\n",
      "Dataset Phoneme not in input folder!\n",
      "Dataset PigArtPressure not in input folder!\n",
      "Dataset Rock not in input folder!\n",
      "Dataset mitbih not in input folder!\n",
      "Dataset ptbdb not in input folder!\n",
      "Dataset etth not in input folder!\n",
      "Dataset ettm not in input folder!\n"
     ]
    }
   ],
   "source": [
    "val_data, val_weights = load_series(val_datasets, \"VAL\", \"../data/processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset australian_electricity_demand_dataset not in input folder!\n",
      "Dataset electricity_hourly_dataset not in input folder!\n",
      "Dataset electricity_load_diagrams not in input folder!\n",
      "Dataset HouseholdPowerConsumption1 not in input folder!\n",
      "Dataset london_smart_meters_dataset_without_missing_values not in input folder!\n",
      "Dataset solar_10_minutes_dataset not in input folder!\n",
      "Dataset wind_farms_minutely_dataset_without_missing_values not in input folder!\n",
      "Dataset ACSF1 not in input folder!\n",
      "Dataset HouseTwenty not in input folder!\n",
      "Dataset Mallat not in input folder!\n",
      "Dataset MixedShapesRegularTrain not in input folder!\n",
      "Dataset Phoneme not in input folder!\n",
      "Dataset PigArtPressure not in input folder!\n",
      "Dataset Rock not in input folder!\n",
      "Dataset mitbih not in input folder!\n",
      "Dataset ptbdb not in input folder!\n",
      "Dataset etth not in input folder!\n",
      "Dataset ettm not in input folder!\n",
      "Dataset australian_electricity_demand_dataset not in input folder!\n",
      "Dataset electricity_hourly_dataset not in input folder!\n",
      "Dataset electricity_load_diagrams not in input folder!\n",
      "Dataset HouseholdPowerConsumption1 not in input folder!\n",
      "Dataset london_smart_meters_dataset_without_missing_values not in input folder!\n",
      "Dataset solar_10_minutes_dataset not in input folder!\n",
      "Dataset wind_farms_minutely_dataset_without_missing_values not in input folder!\n",
      "Dataset ACSF1 not in input folder!\n",
      "Dataset HouseTwenty not in input folder!\n",
      "Dataset Mallat not in input folder!\n",
      "Dataset MixedShapesRegularTrain not in input folder!\n",
      "Dataset Phoneme not in input folder!\n",
      "Dataset PigArtPressure not in input folder!\n",
      "Dataset PigCVP not in input folder!\n",
      "Dataset Rock not in input folder!\n",
      "Dataset SemgHandGenderCh2 not in input folder!\n",
      "Dataset mitbih not in input folder!\n",
      "Dataset ptbdb not in input folder!\n",
      "Dataset etth not in input folder!\n",
      "Dataset ettm not in input folder!\n"
     ]
    }
   ],
   "source": [
    "train_data, train_weights = load_series(val_datasets, \"TRAIN\", \"../data/processed\")\n",
    "val_data, val_weights = load_series(val_datasets, \"VAL\", \"../data/peocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset australian_electricity_demand_dataset not in input folder!\n",
      "Dataset electricity_hourly_dataset not in input folder!\n",
      "Dataset electricity_load_diagrams not in input folder!\n",
      "Dataset HouseholdPowerConsumption1 not in input folder!\n",
      "Dataset london_smart_meters_dataset_without_missing_values not in input folder!\n",
      "Dataset solar_10_minutes_dataset not in input folder!\n",
      "Dataset wind_farms_minutely_dataset_without_missing_values not in input folder!\n",
      "Dataset ACSF1 not in input folder!\n",
      "Dataset HouseTwenty not in input folder!\n",
      "Dataset Mallat not in input folder!\n",
      "Dataset MixedShapesRegularTrain not in input folder!\n",
      "Dataset Phoneme not in input folder!\n",
      "Dataset PigArtPressure not in input folder!\n",
      "Dataset Rock not in input folder!\n",
      "Dataset mitbih not in input folder!\n",
      "Dataset ptbdb not in input folder!\n",
      "Dataset etth not in input folder!\n",
      "Dataset ettm not in input folder!\n"
     ]
    }
   ],
   "source": [
    "input_path = \"../data/processed\"\n",
    "\n",
    "if not val_file.exists():\n",
    "    data, weights = load_series(val_datasets, split=\"VAL\", path=input_path)\n",
    "    val_gen = CenteredArtifactDataset(data,\n",
    "                              width=CONFIG[\"width\"],\n",
    "                              padding=16,\n",
    "                              artifact=Saw_centered_Francois(),\n",
    "                              weight=weights\n",
    "                              )\n",
    "    val = CachedArtifactDataset.generate(val_gen,\n",
    "                                         n=2048,\n",
    "                                         to=val_file)\n",
    "else:\n",
    "    val = CachedArtifactDataset(file=val_file)\n",
    "val_loader = DataLoader(val, batch_size=CONFIG[\"batch_size\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
